{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c6bf05-2e31-46b6-8a24-6e8bac97382d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# PyTorch\n",
    "\n",
    "[PyTorch](https://en.wikipedia.org/wiki/PyTorch) is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing. PyTorch provides two high-level features:\n",
    "\n",
    "- Tensor computing with acceleration via graphics processing units (GPUs)\n",
    "- Deep neural networks built on a tape-based automatic differentiation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f044dec-3842-4719-af51-781875b9c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb2939-a279-4162-b8f1-0b51b521235a",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "The central data abstraction in PyTorch is given by the `torch.tensor` class. It represents the counterpart of the `numpy.ndarray` class in NumPy, and many of the respective class methods have similar syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ecc5c-383a-4d12-8080-e74895567339",
   "metadata": {},
   "source": [
    "### Tensor creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96366641-264e-4043-8f5c-d073ed9edb35",
   "metadata": {},
   "source": [
    "Ways to create PyTorch tensors include:\n",
    "\n",
    "- `torch.tensor()`\n",
    "- `torch.empty()`\n",
    "- `torch.zeros()`\n",
    "- `torch.ones()`\n",
    "- `torch.rand()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eccc351-9363-4820-be60-bf22b527ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3, 3, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a7c974-c503-4868-acf1-dd4f07f91bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6432, 0.9829, 0.7563],\n",
      "        [0.6980, 0.9340, 0.5951],\n",
      "        [0.5386, 0.4091, 0.6741]])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310b3be-bf97-4ffd-992c-336a51a10896",
   "metadata": {},
   "source": [
    "By default, PyTorch tensors are populated with 32-bit (single precision) floating point numbers suitable for arithmetic operations on GPUs, but many other data types are available and include:\n",
    "\n",
    "- `torch.bool`\n",
    "- `torch.int8`\n",
    "- `torch.int16`\n",
    "- `torch.int32`\n",
    "- `torch.int64`\n",
    "- `torch.half` or `torch.float16`\n",
    "- `torch.float`\n",
    "- `torch.double` or `torch.float64`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaabecd-9cf0-4946-9bc5-129b989b2472",
   "metadata": {},
   "source": [
    "A PyTorch tensor can be converted to a regular Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f18192-e2d7-4ca6-897d-6a7821306aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6432363986968994, 0.9828986525535583, 0.7563287019729614],\n",
       " [0.6980193853378296, 0.9339519143104553, 0.595119297504425],\n",
       " [0.5386022329330444, 0.40910327434539795, 0.6740598082542419]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6175487-afd5-4275-9636-9478f001c5da",
   "metadata": {},
   "source": [
    "Conversely, a Python list can be converted to a PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88abbc97-078a-454a-b2be-9ebabd727162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6432, 0.9829, 0.7563],\n",
       "        [0.6980, 0.9340, 0.5951],\n",
       "        [0.5386, 0.4091, 0.6741]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(a.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e0d98-581a-4578-a4e0-8d3254699069",
   "metadata": {},
   "source": [
    "### Tensor operations\n",
    "\n",
    "PyTorch tensors have over three hundred operations that can be performed on them, including:\n",
    "\n",
    "- `torch.abs()`\n",
    "- `torch.max()`\n",
    "- `torch.mean()`\n",
    "- `torch.std()`\n",
    "- `torch.prod()`\n",
    "- `torch.unique()`\n",
    "- `torch.matmul()`\n",
    "- `torch.svd()`\n",
    "- `torch.sin()`\n",
    "- `torch.cos()`\n",
    "- `torch.flatten()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa48c00b-0b64-49c9-9a1a-5519930c21ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6924)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e0309c-93df-4509-8867-e3a6f4b80a8e",
   "metadata": {},
   "source": [
    "Note that a tensor with a scalar number is given in return. To instead get a Python number in return, we can perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca4d6f69-69a2-4531-9326-4c34125c3d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923689246177673"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8802d60a-d627-4247-ab5d-83f97642def9",
   "metadata": {},
   "source": [
    "### NumPy bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5d2bf4-cb2c-43a8-aa2c-8cbb741340e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51233599-9c2d-4c5e-aa27-48cb2a79ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.ones((2, 3))\n",
    "pth_tensor = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "498c2ae8-55b3-4760-85ff-dd502be0615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(pth_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa5f1f5-299f-4b7c-82c8-82540e71f047",
   "metadata": {},
   "source": [
    "We note that the NumPy array default data type of float64 (double precision) is preserved. In fact, we merely created a pointer to the same data in memory such that a change in one object is reflected in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f44bd8d-3e35-4a37-a17b-44bb600e9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array[1, 2] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bee37edc-ac34-41aa-81bd-b072c35241ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified numpy array:\n",
      " [[1. 1. 1.]\n",
      " [1. 1. 2.]]\n",
      "Bridged pytorch tensor:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Modified numpy array:\\n\", np_array)\n",
    "print(\"Bridged pytorch tensor:\\n\", pth_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1471e1c6-44ec-434d-a292-bd767ddafba4",
   "metadata": {},
   "source": [
    "A reason to create a bridge between data can e.g. be to take advantage of the easy accessible  GPU acceleration available in PyTorch for scientific codes developed with NumPy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd2249-5563-47a5-9120-f80601180f15",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46be380-f7a3-4d45-a50e-e6db2f76a68e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The machine learning models in PyTorch are built as neural networks with layers of neurons. Every neuron has an associated activation level.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d91b9-fa63-4284-86dc-4b721e50aab7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![Neural Network](../images/neural_network.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ed39d-7da6-42a6-b574-4c278d487cc8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The input layer receives input data; hidden layers transform the data; and the output layer provides the results upon which model predictions are made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd19dd-edc9-461a-bdb3-b4b7f4e96491",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The input level apart, activation levels in a given level, say $L$, are determined from those in the previous layer by use of weights that are collected in a matrix $\\boldsymbol{W}^{(L)}$ and biases that are collected in a row vector $\\boldsymbol{b}^{(L)}$. \n",
    "\n",
    "The organization of the weights into matrix form is illustrated in figure. The neuron number in layer $L$ becomes the row index and the neuron number in layer $L-1$ becomes the column index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057e8e4-42de-43cd-b362-d55c1e639408",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "A layer is referred to as *linear* if the weights and biases are applied in a linear transformation\n",
    "\n",
    "$$\n",
    "\\boldsymbol{a}^{(L)} = f(\\boldsymbol{a}^{(L-1)} \\big[\\boldsymbol{W}^{(L)}\\big]^T \n",
    "+ \\boldsymbol{b}^{(L)})\n",
    "$$\n",
    "\n",
    "As indicated, to get the final activation levels also involves the elementwise operation of a  (typically) nonlinear activation function, $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4fa89b-8e6f-425d-8e35-28c73084e305",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "When instantiated, layer $L$ receives weight and bias attributes that are initialized randomly with values\n",
    "\n",
    "$$\n",
    "-1/\\sqrt{n_{L-1}} < w_{ij}^{(L)}, b_i^{(L)} < 1 / \\sqrt{n_{L-1}}\n",
    "$$\n",
    "\n",
    "where $n_{L-1}$ is the number of neurons in layer $L-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49c7c4a4-ff41-46b1-b5e1-ee691c1e5b06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x106f03b90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(20240305)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e621958e-a481-4365-ad84-062d8621be8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Input layer\n",
    "\n",
    "Let us assume that we have the following input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8036a2e0-0e19-4b9f-bc4c-51046a4ea048",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer data:\n",
      " tensor([2.8317, 0.7713, 0.7910])\n"
     ]
    }
   ],
   "source": [
    "a0 = torch.tensor([2.8317, 0.7713, 0.7910])\n",
    "\n",
    "print(\"Input layer data:\\n\", a0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9799a-b95b-4d27-a703-65878e97ba0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Hidden layer\n",
    "\n",
    "#### Layer transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c80033-0f7e-4410-bdc5-16f545b8f20a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The linear layer transformation is achieved with the `torch.nn.Linear` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673e262-7b7c-4be8-bb3e-34c111ba874c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here we consider a transformation from an input layer with three neurons, $n_0 = 3$, to a hidden layer with four neurons, $n_1 = 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de014504-c10b-4909-8a0a-c4a0d885a33e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden = torch.nn.Linear(3, 4, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001bf868-97ce-41cc-aaa5-b4754bfbe098",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The weights and biases are available as attributes of the layer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e31176b8-a828-4448-af13-960a2b663e73",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3830,  0.3132, -0.4861],\n",
       "        [ 0.3464, -0.4345, -0.4673],\n",
       "        [ 0.0303,  0.3445,  0.0182],\n",
       "        [-0.1550,  0.4523, -0.1824]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0cefe79-513f-461a-8556-0fbd8201bae1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1304, -0.0389, -0.1370, -0.2898], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0698d17-e8da-4450-b85c-3dec87c97e65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We now use PyToch to perform the layer transformation of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "103619e2-ef99-470b-9bda-75aeb9ae30b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8114,  0.2372,  0.2288, -0.5243], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden(a0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0301857-d985-4791-ac59-b02e73e40d33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We check the transformation with an explicit calculation of the linear transformation:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{a}^{(0)} \\big[\\boldsymbol{W}^{(1)}\\big]^T \n",
    "+ \\boldsymbol{b}^{(1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0cda628-033f-4d8b-b9d1-6207c06f02f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8114,  0.2372,  0.2288, -0.5243], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a0, hidden.weight.T) + hidden.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861c38e-1151-447b-8ff3-7791db8ce12e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We note that the two results are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf644a12-289e-46c7-b3d3-6237d5034a2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Activation function\n",
    "\n",
    "Now remains the application of the nonlinear activation function, $f$, according to\n",
    "\n",
    "$$\n",
    "\\boldsymbol{a}^{(1)} =f( \n",
    "\\boldsymbol{a}^{(0)} \\big[\\boldsymbol{W}^{(1)}\\big]^T \n",
    "+ \\boldsymbol{b}^{(1)})\n",
    "$$\n",
    "\n",
    "A common choice in machine learning is to adopt the rectifier linear unit function\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(x) = \\max(0,x) = \\frac{x + |x|}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1819c3ec-40a6-4cb9-bc97-0c58ecbd65ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97af27bc-8774-4d6c-8b5d-bbe46e937fa9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer data:\n",
      " tensor([0.8114, 0.2372, 0.2288, 0.0000], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a1 = relu(hidden(a0))\n",
    "\n",
    "print(\"Hidden layer data:\\n\", a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d3f73-e377-4680-8d47-34c5b0f6a144",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The effect of the ReLU function is as anticipated, turning activation level $a^{(1)}_3$ to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78228633-3b65-4981-bcf9-8f1d9e6c94f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a7187-4ab4-49ac-859c-4d2bdb313d21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We create the output layer as a linear layer without a nonlinear activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22b79cf9-29a7-4588-b571-c2d4ab82d386",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = torch.nn.Linear(4, 2, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af1161a4-c477-4a55-9214-6e53992cb649",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer data:\n",
      " tensor([-0.1211, -0.2292], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a2 = output(a1)\n",
    "\n",
    "print(\"Output layer data:\\n\", a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27bfb1b-f2d8-4db2-b4a2-0096f79033cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32ef4d4b-307d-4803-b0e4-bc3068ed8a0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = torch.tensor([-0.5, -1.0])\n",
    "\n",
    "size = len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6fd8f-bd1e-41d6-8c14-d02e22c3e664",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae393cba-e7c1-4cf9-9786-ced6924bf6a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In the process of training the network, we need a measure of closeness between the prediction in the output layer and the correct result. This measure is given by a *loss function*. Several [loss functions are available in PyTorch](https://pytorch.org/docs/stable/nn.html#loss-functions) for different purposes. We will here adopt the *mean square error* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53f1e28c-18ae-4584-b91d-afa4d9c580ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_mse = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21b3e5ce-8116-41d6-b71a-0a0e9be0200e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss based on mean square error:\n",
      " tensor(0.3688, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_mse(a2, training_data)\n",
    "\n",
    "print(\"Loss based on mean square error:\\n\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d5797aa-a491-480e-8420-70ee4c15d5de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3688, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((a2 - training_data) ** 2) / size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bd4c84-2f12-4145-9d8c-48a8eacb0272",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b6c4f2-3faf-4c42-8de6-37bfb7edcc59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The gradient of the loss function with respect to weight and bias parameters are determined with a method known as [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) that is based on chain rule differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a9029da-f97d-4632-a0a3-c31680bdfece",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "887b09a7-c407-4536-afe1-e14753bb3a3d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7216, -0.1966, -0.2016],\n",
       "        [ 0.0070,  0.0019,  0.0019],\n",
       "        [ 1.2845,  0.3499,  0.3588],\n",
       "        [ 0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeaca2c-6e04-4bbf-91ba-47892db312a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We note that since activation level $a_3^{(1)}$ became equal to zero in our network, the gradient with respect to weight parameters $w^{(1)}_{30}$, $w^{(1)}_{31}$, and $w^{(1)}_{32}$ vanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71a2da72-abb2-40e9-88e4-6d2aff79226e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3074, 0.0899, 0.0867, 0.0000],\n",
       "        [0.6254, 0.1828, 0.1764, 0.0000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8564d6e6-f7bc-425e-ae7c-44ebf4d6bbd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We note that since activation level $a_3^{(1)}$ became equal to zero in our network, the gradient with respect to weight parameters $w^{(2)}_{03}$ and $w^{(2)}_{13}$ vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d317aeb-a450-4b11-a552-2f6f21a22a96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "With access to these gradients, the parameters can be modified in a way to reduce the value of the loss function. This iterative process is referred to as training the network.\n",
    "\n",
    "A large training data set is required in practice and an approach such as the [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) method can be used to update the parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109085ac-77f1-4c8e-9e11-fa69f453703d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Cross entropy loss function\n",
    "In binary classification networks, the `CrossEntropyLoss()` function is a typical choice. The evaluation of this loss function is a bit less straightforward and since it will be subsequently used, it is here illustrated by an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f363818-4eab-40c3-9405-c756185e7223",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1597eeb8-d9fc-40dd-b459-59e2dd52438a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let us assume that we have four classes in the output layer and that we are concerned with a specific item in the data set for which the correct answer is class number three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7f95160-ceb3-4ceb-91aa-89fd2b703663",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct_answer = torch.tensor([0.0, 0.0, 1.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb851b-0799-46ac-a84c-1b72d8f75607",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let us further assume that we have made two separate predictions (one good and one bad) in the output layer leading to the following activity levels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f7bbee0-5c3e-483f-b80c-2793c5a631bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "good_prediction = torch.tensor([0.2, 0.5, 3.1, -0.1])\n",
    "\n",
    "bad_prediction = torch.tensor([2.0, 2.5, 1.1, -0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8153d4-e935-4138-91b7-5e9c0a215b9b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The associated loss function values (errors) are given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2829799e-8f82-40d8-ace3-2d6c7fda0f5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good prediction loss = tensor(0.1571)\n",
      "bad prediction loss  = tensor(2.0434)\n"
     ]
    }
   ],
   "source": [
    "print(\"good prediction loss =\", loss_func(good_prediction, correct_answer))\n",
    "print(\"bad prediction loss  =\", loss_func(bad_prediction, correct_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b8df4-8ada-4d87-a891-a68419bb37d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As expected, the error is deemed much larger for the bad prediction.\n",
    "\n",
    "Let us see how PyTorch came this conclusion. \n",
    "\n",
    "In a first step, the predictions are exponentialized, promoting large positive numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7a369fb-5afa-44be-a055-506b3b254b57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: good prediction loss = tensor([ 1.2214,  1.6487, 22.1979,  0.9048])\n",
      "step 1: bad prediction loss  = tensor([ 7.3891, 12.1825,  3.0042,  0.6065])\n"
     ]
    }
   ],
   "source": [
    "good_p1 = torch.exp(good_prediction)\n",
    "bad_p1 = torch.exp(bad_prediction)\n",
    "\n",
    "print(\"step 1: good prediction loss =\", good_p1)\n",
    "print(\"step 1: bad prediction loss  =\", bad_p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a0f3ee-88c6-4292-99be-4fc5461e3d71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In a second step, a normalization is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "432eecb2-a01d-48ba-bcf1-babddc99f898",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2: good prediction loss = tensor([0.0470, 0.0635, 0.8547, 0.0348])\n",
      "step 2: bad prediction loss  = tensor([0.3187, 0.5255, 0.1296, 0.0262])\n"
     ]
    }
   ],
   "source": [
    "good_p2 = good_p1 / good_p1.sum()\n",
    "bad_p2 = bad_p1 / bad_p1.sum()\n",
    "\n",
    "print(\"step 2: good prediction loss =\", good_p2)\n",
    "print(\"step 2: bad prediction loss  =\", bad_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccba709-be66-404f-a6ae-faa1d35177eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In a third step, we take the negative logarithm so that a values close to one become close to zero (low loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0d8d12d-d343-4bb2-8a3c-dfb88f6aef28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: good prediction loss = tensor([3.0571, 2.7571, 0.1571, 3.3571])\n",
      "step 3: bad prediction loss  = tensor([1.1434, 0.6434, 2.0434, 3.6434])\n"
     ]
    }
   ],
   "source": [
    "good_p3 = -torch.log(good_p2)\n",
    "bad_p3 = -torch.log(bad_p2)\n",
    "\n",
    "print(\"step 3: good prediction loss =\", good_p3)\n",
    "print(\"step 3: bad prediction loss  =\", bad_p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21584dc-626f-458b-9192-16281f60e9b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In a forth step, we pick out the loss for the binary correct answer by means of a product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59f3e4a0-f2d2-425a-b3ac-3819f1186712",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: good prediction loss = tensor([0.0000, 0.0000, 0.1571, 0.0000])\n",
      "step 4: bad prediction loss  = tensor([0.0000, 0.0000, 2.0434, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "good_p4 = good_p3 * correct_answer\n",
    "bad_p4 = bad_p3 * correct_answer\n",
    "\n",
    "print(\"step 4: good prediction loss =\", good_p4)\n",
    "print(\"step 4: bad prediction loss  =\", bad_p4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfdd63e-a714-482e-a0f9-dfebc84dd5cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In a fifth step, a summation is performed to produce a scalar loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c7ed5ce-98b5-4dbc-b318-1349337a61ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good prediction loss = tensor(0.1571)\n",
      "bad prediction loss  = tensor(2.0434)\n"
     ]
    }
   ],
   "source": [
    "print(\"good prediction loss =\", good_p4.sum())\n",
    "print(\"bad prediction loss  =\", bad_p4.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea2bacb-5902-432f-95ac-6bf4671a3c92",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We note that the resulting losses are identical to those obtained with the PyTorch loss function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
